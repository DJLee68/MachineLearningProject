{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment10/DJLee_Assignment10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9dF5bk7sEjnfeZxZTnKFv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJLee68/MachineLearningProject/blob/master/Assignment10/DJLee_Assignment10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6MJQZZMN6AQ",
        "outputId": "53c01a22-d92c-4762-b96a-e950cf07b943"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, utils, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,),(0.3081,)),  # mean value = 0.1307, standard deviation value = 0.3081\n",
        "])\n",
        "\n",
        "data_path = './MNIST'\n",
        "\n",
        "train_data = datasets.MNIST(root = data_path, train= False, download=True, transform= transform)\n",
        "test_data = datasets.MNIST(root = data_path, train= True, download=True, transform= transform)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fdSb6XKnkvr",
        "outputId": "bace0784-8897-47f1-db0b-db88550e26f7"
      },
      "source": [
        "print(\"the number of your training data (must be 10,000) = \", train_data.__len__())\n",
        "print(\"hte number of your testing data (must be 60,000) = \", test_data.__len__())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the number of your training data (must be 10,000) =  10000\n",
            "hte number of your testing data (must be 60,000) =  60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuaW21NdrEBq"
      },
      "source": [
        " class Classification(nn.Module):\n",
        "    def __init__(self, param1, param2, dr_rate=0):\n",
        "        super(Classification, self).__init__()\n",
        "        self.param1 = param1\n",
        "        self.param2 = param2\n",
        "        # construct layers for a neural network\n",
        "        self.classifier1 = nn.Sequential(\n",
        "            nn.Linear(in_features=28*28, out_features=self.param1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dr_rate),\n",
        "        ) \n",
        "        self.classifier2 = nn.Sequential(\n",
        "            nn.Linear(in_features=self.param1, out_features=self.param2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dr_rate),\n",
        "        ) \n",
        "        self.classifier3 = nn.Sequential(\n",
        "            nn.Linear(in_features=self.param2, out_features=10),\n",
        "            nn.LogSoftmax(dim=1),\n",
        "        ) \n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):                 # [batchSize, 1, 28, 28]\n",
        "        x = inputs.view(inputs.size(0), -1)    # [batchSize, 28*28]\n",
        "        x = self.classifier1(x)                # [batchSize, 20*20]\n",
        "        x = self.classifier2(x)                # [batchSize, 10*10]\n",
        "        out = self.classifier3(x)              # [batchSize, 10]\n",
        "        \n",
        "        return out\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nV6LMhxumo5"
      },
      "source": [
        "lr=0.0001\n",
        "\n",
        "classification = Classification(10,10,0).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classification.parameters(), lr=lr)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rTxmHUV1nS2"
      },
      "source": [
        "def run_epoch (train_data, test_data):\n",
        "    \n",
        "    tr_loss = 0\n",
        "    tr_acc = 0\n",
        "    iter = len(train_data)\n",
        "    classification.train()\n",
        "    for img_i, label_i in train_data: #{\n",
        "        img_i, label_i = img_i.to(device), label_i.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Forward\n",
        "        y_pred = classification.forward(img_i.view(-1, 28*28))\n",
        "        ps = torch.exp(y_pred)\n",
        "        correct = (label_i == ps.max(dim=1)[1])\n",
        "        # Loss computation\n",
        "        loss  = criterion(y_pred, label_i)\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        # Optimize for img_i\n",
        "        optimizer.step()\n",
        "        tr_loss += loss.item()\n",
        "        tr_acc += correct.type(torch.FloatTensor).mean()\n",
        "    #}\n",
        "    tr_loss /= iter\n",
        "    tr_acc /= iter\n",
        "    \n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    iter_test = len(test_data)\n",
        "\n",
        "    classification.eval()\n",
        "    for img_j, label_j in test_data:\n",
        "        img_j, label_j = img_j.to(device), label_j.to(device)\n",
        "        correct = 0\n",
        "        with torch.autograd.no_grad():\n",
        "            predicted = classification.forward(img_j.view(-1, 28*28))\n",
        "            ps = torch.exp(predicted)\n",
        "            correct = (label_j == ps.max(dim=1)[1])\n",
        "            test_loss += criterion(predicted, label_j).item()\n",
        "            test_acc += correct.type(torch.FloatTensor).mean()\n",
        "    \n",
        "    test_loss /= iter_test\n",
        "    test_acc /= iter_test\n",
        "    return tr_loss, tr_acc, test_loss, test_acc\n",
        "\n",
        "\n",
        "final_train_loss = []\n",
        "final_test_loss = []\n",
        "final_train_acc = []\n",
        "final_test_acc = []\n",
        "\n",
        "def run(batch_size, epochs, param1, param2, dr_rate): #{\n",
        "  global optimizer, criterion, classification\n",
        "  classification = Classification(param1, param2, dr_rate).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(classification.parameters(), lr=lr)\n",
        "\n",
        "  global final_train_loss, final_test_loss, final_train_acc, final_test_acc\n",
        "  train_data_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=False)\n",
        "  test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)\n",
        "  mini_batch_data, mini_batch_label  = next(iter(train_data_loader))\n",
        "  \n",
        "  train_loss = []\n",
        "  test_loss = []\n",
        "  train_acc = []\n",
        "  test_acc = []\n",
        "\n",
        "  for epoch in range(epochs): #{\n",
        "    tr_loss, tr_acc, te_loss, te_acc = run_epoch(train_data_loader, test_data_loader)\n",
        "    train_loss.append(tr_loss)\n",
        "    train_acc.append(tr_acc)\n",
        "    test_loss.append(te_loss)\n",
        "    test_acc.append(te_acc)\n",
        "  #}\n",
        "\n",
        "  final_train_loss.append(train_loss[-1])\n",
        "  final_test_loss.append(test_loss[-1])\n",
        "  final_train_acc.append(train_acc[-1])\n",
        "  final_test_acc.append(test_acc[-1])\n",
        "\n",
        "  return train_loss, test_loss, train_acc, test_acc\n",
        "\n",
        "#}\n",
        "\n",
        "# train_loss_32, test_loss_32, train_acc_32, test_acc_32 = run(32, 200,2048,1024, 0.2)\n",
        "train_loss_32, test_loss_32, train_acc_32, test_acc_32 = run(32, 200,1024,512, 0.2)\n",
        "train_loss_32, test_loss_32, train_acc_32, test_acc_32 = run(32, 200,1024,512, 0.1)\n",
        "\n",
        "# Plot image\n",
        "plt.figure(0, figsize=(10,6))\n",
        "plt.plot(train_loss_32, label='train loss', c='r')\n",
        "plt.plot(test_loss_32, label='test loss', c='b')\n",
        "plt.title(f'Batch 32 loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(1, figsize=(10,6))\n",
        "plt.plot(train_acc_32, label='train accuracy', c='r')\n",
        "plt.plot(test_acc_32, label='test accuracy', c='b')\n",
        "plt.title(f'Batch 32 accuracy')\n",
        "plt.legend()\n",
        "plt.show()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8NTUbXBGA1p"
      },
      "source": [
        "## Output using the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loBPs6M4GBlZ"
      },
      "source": [
        " 1. Plot the training and testing losses with a batch size of 32 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-rOmexxF-GI"
      },
      "source": [
        "plt.figure(0, figsize=(10,6))\n",
        "plt.plot(train_loss_32, label='train loss', c='r')\n",
        "plt.plot(test_loss_32, label='test loss', c='b')\n",
        "plt.title(f'Batch 32 loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sshlJEPGCcL"
      },
      "source": [
        " 2. Plot the training and testing accuracies with a batch size of 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPNzNIGkF-UH"
      },
      "source": [
        "plt.figure(1, figsize=(10,6))\n",
        "plt.plot(train_acc_32, label='train accuracy', c='r')\n",
        "plt.plot(test_acc_32, label='test accuracy', c='b')\n",
        "plt.title(f'Batch 32 accuracy')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxpBSh_GDpP"
      },
      "source": [
        "3. Plot the training and testing losses with a batch size of 64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ek0PZLPF-cg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnBO_W3wGETH"
      },
      "source": [
        "4. Plot the training and testing accuracies with a batch size of 64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rli7AuSBF-h0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMz1y3ZoGFMF"
      },
      "source": [
        "5. Plot the training and testing losses with a batch size of 128 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AYIrbscF-mb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqdD6NRpGFsa"
      },
      "source": [
        "6. Plot the training and testing accuracies with a batch size of 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GH7gutfF-qQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhyVvX7OGGPj"
      },
      "source": [
        "7. Print the loss at convergence with different mini-batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuYE2x0mF-ue"
      },
      "source": [
        "for i in final_train_loss:\n",
        "  print(f'1{i}')\n",
        "\n",
        "for i in final_train_acc:\n",
        "  print(f'2{i}')\n",
        "\n",
        "for i in final_test_loss:\n",
        "  print(f'3{i}')\n",
        "for i in final_test_acc:\n",
        "  print(f'4{i}') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boKtMA_9Gg3K"
      },
      "source": [
        "8. Print the accuracy at convergence with different mini-batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2KFW4t5F-xy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg9ct7DNnj0y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}